<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<title>Markmap</title>
<style>
* {
  margin: 0;
  padding: 0;
}
#mindmap {
  display: block;
  width: 100vw;
  height: 100vh;
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.17.2/dist/style.css">
</head>
<body>
<svg id="mindmap"></svg>
<script src="https://cdn.jsdelivr.net/npm/d3@7.8.5/dist/d3.min.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-view@0.17.2/dist/browser/index.js"></script><script src="https://cdn.jsdelivr.net/npm/markmap-toolbar@0.17.2/dist/index.js"></script><script>(r => {
                setTimeout(r);
              })(() => {
  const {
    markmap,
    mm
  } = window;
  const {
    el
  } = markmap.Toolbar.create(mm);
  el.setAttribute('style', 'position:absolute;bottom:20px;right:20px');
  document.body.append(el);
})</script><script>((getMarkmap, getOptions, root2, jsonOptions) => {
              const markmap = getMarkmap();
              window.mm = markmap.Markmap.create(
                "svg#mindmap",
                (getOptions || markmap.deriveOptions)(jsonOptions),
                root2
              );
            })(() => window.markmap,null,{"content":"Algoritmos mais Usados em Ciência de Dados","children":[{"content":"1. Algoritmos de Aprendizado Supervisionado","children":[{"content":"Classificação","children":[{"content":"<strong>Regressão Logística</strong>: Utilizado para problemas de classificação binária, modelando a probabilidade de uma variável dependente ser 1 ou 0.","children":[],"payload":{"lines":"5,6"}},{"content":"<strong>Árvores de Decisão</strong>: Constrói uma árvore onde cada nó representa uma decisão com base em características, até chegar a um resultado final (classe).","children":[],"payload":{"lines":"6,7"}},{"content":"<strong>Random Forest</strong>: Um ensemble de árvores de decisão, onde a combinação de múltiplas árvores melhora a precisão da classificação.","children":[],"payload":{"lines":"7,8"}},{"content":"<strong>Support Vector Machines (SVM)</strong>: Utiliza um hiperplano para separar classes, buscando maximizar a margem entre as classes.","children":[],"payload":{"lines":"8,9"}},{"content":"<strong>K-Nearest Neighbors (KNN)</strong>: Classifica um ponto com base nas classes mais comuns entre os 'k' vizinhos mais próximos.","children":[],"payload":{"lines":"9,10"}},{"content":"<strong>Naive Bayes</strong>: Baseado no Teorema de Bayes, assume que as características são independentes, muito usado em problemas de texto.","children":[],"payload":{"lines":"10,11"}},{"content":"<strong>Gradient Boosting Machines (GBM)</strong>: Combina múltiplos modelos fracos, como árvores de decisão, para criar um modelo forte.","children":[],"payload":{"lines":"11,12"}},{"content":"<strong>XGBoost</strong>: Uma implementação eficiente de Gradient Boosting, muito usado em competições de machine learning por sua precisão.","children":[],"payload":{"lines":"12,13"}},{"content":"<strong>LightGBM</strong>: Um framework baseado em árvore de decisão que usa uma técnica de histograma para acelerar o treinamento.","children":[],"payload":{"lines":"13,14"}},{"content":"<strong>CatBoost</strong>: Semelhante ao XGBoost e LightGBM, mas com uma ênfase na eficiência em dados categóricos.","children":[],"payload":{"lines":"14,16"}}],"payload":{"lines":"4,5"}},{"content":"Regressão","children":[{"content":"<strong>Regressão Linear</strong>: Modela a relação entre uma variável dependente e uma ou mais independentes, assumindo uma relação linear.","children":[],"payload":{"lines":"17,18"}},{"content":"<strong>Regressão Ridge</strong>: Regressão linear com uma penalização para reduzir a complexidade do modelo e evitar overfitting.","children":[],"payload":{"lines":"18,19"}},{"content":"<strong>Regressão Lasso</strong>: Semelhante à regressão Ridge, mas com a capacidade de realizar seleção de variáveis, tornando alguns coeficientes zero.","children":[],"payload":{"lines":"19,20"}},{"content":"<strong>Regressão ElasticNet</strong>: Combina a penalização de Ridge e Lasso para melhorar a generalização.","children":[],"payload":{"lines":"20,21"}},{"content":"<strong>Support Vector Regression (SVR)</strong>: Utiliza um modelo SVM para problemas de regressão, tentando encontrar um hiperplano que minimize os erros.","children":[],"payload":{"lines":"21,23"}}],"payload":{"lines":"16,17"}}],"payload":{"lines":"2,3"}},{"content":"2. Algoritmos de Aprendizado Não Supervisionado","children":[{"content":"Clustering (Agrupamento)","children":[{"content":"<strong>K-Means</strong>: Divide os dados em 'k' clusters, minimizando a soma das distâncias dos pontos aos centros dos clusters.","children":[],"payload":{"lines":"28,29"}},{"content":"<strong>DBSCAN</strong>: Identifica clusters de pontos densos e pode encontrar formas arbitrárias, não necessita de definição prévia do número de clusters.","children":[],"payload":{"lines":"29,30"}},{"content":"<strong>Hierarchical Clustering (Aglomerativo)</strong>: Cria uma hierarquia de clusters que pode ser visualizada em um dendrograma.","children":[],"payload":{"lines":"30,31"}},{"content":"<strong>Gaussian Mixture Models (GMM)</strong>: Modela os dados como uma combinação de várias distribuições gaussianas.","children":[],"payload":{"lines":"31,32"}},{"content":"<strong>Agglomerative Clustering</strong>: Tipo de clustering hierárquico que começa com cada ponto como um cluster e os combina progressivamente.","children":[],"payload":{"lines":"32,33"}},{"content":"<strong>Mean Shift</strong>: Algoritmo de clustering que tenta encontrar a densidade máxima de pontos em uma região para formar clusters.","children":[],"payload":{"lines":"33,35"}}],"payload":{"lines":"27,28"}},{"content":"Redução de Dimensionalidade","children":[{"content":"<strong>PCA (Principal Component Analysis)</strong>: Reduz a dimensionalidade dos dados projetando-os em um espaço de componentes principais.","children":[],"payload":{"lines":"36,37"}},{"content":"<strong>t-SNE (t-Distributed Stochastic Neighbor Embedding)</strong>: Técnica de visualização de dados de alta dimensão, preservando a proximidade entre pontos similares.","children":[],"payload":{"lines":"37,38"}},{"content":"<strong>UMAP (Uniform Manifold Approximation and Projection)</strong>: Método de redução de dimensionalidade semelhante ao t-SNE, mas mais rápido e eficiente em grandes conjuntos de dados.","children":[],"payload":{"lines":"38,39"}},{"content":"<strong>LDA (Linear Discriminant Analysis)</strong>: Técnica de redução de dimensionalidade que também tenta maximizar a separação entre diferentes classes.","children":[],"payload":{"lines":"39,41"}}],"payload":{"lines":"35,36"}}],"payload":{"lines":"25,26"}},{"content":"3. Algoritmos de Aprendizado por Reforço (Reinforcement Learning)","children":[{"content":"<strong>Q-Learning</strong>: Algoritmo que aprende uma política ótima em um ambiente, atualizando o valor das ações com base nas recompensas recebidas.","children":[],"payload":{"lines":"45,46"}},{"content":"<strong>Deep Q-Network (DQN)</strong>: Uma extensão do Q-Learning usando redes neurais profundas para aprender valores de ações.","children":[],"payload":{"lines":"46,47"}},{"content":"<strong>SARSA (State-Action-Reward-State-Action)</strong>: Semelhante ao Q-Learning, mas atualiza os valores com base nas ações realmente tomadas.","children":[],"payload":{"lines":"47,48"}},{"content":"<strong>Policy Gradient</strong>: Otimiza diretamente a política de ação, em vez de estimar valores das ações.","children":[],"payload":{"lines":"48,49"}},{"content":"<strong>Actor-Critic</strong>: Combina elementos de Q-Learning e Policy Gradient, com um ator que escolhe ações e um crítico que avalia a política.","children":[],"payload":{"lines":"49,50"}},{"content":"<strong>A3C (Asynchronous Advantage Actor-Critic)</strong>: Um modelo de aprendizado por reforço assíncrono que melhora a estabilidade e a performance.","children":[],"payload":{"lines":"50,51"}},{"content":"<strong>Proximal Policy Optimization (PPO)</strong>: Método de otimização de políticas que usa uma abordagem conservadora para atualizar a política, evitando grandes mudanças.","children":[],"payload":{"lines":"51,53"}}],"payload":{"lines":"43,44"}},{"content":"4. Algoritmos de Deep Learning","children":[{"content":"Redes Neurais Artificiais (ANN)","children":[{"content":"<strong>Perceptron</strong>: O modelo mais simples de rede neural, composto por um único neurônio.","children":[],"payload":{"lines":"58,59"}},{"content":"<strong>Feedforward Neural Network (FNN)</strong>: Rede neural com camadas totalmente conectadas, onde a informação flui de entrada até a saída.","children":[],"payload":{"lines":"59,61"}}],"payload":{"lines":"57,58"}},{"content":"Redes Neurais Convolucionais (CNN)","children":[{"content":"<strong>LeNet</strong>: Uma das primeiras redes neurais convolucionais, usada para reconhecer dígitos manuscritos.","children":[],"payload":{"lines":"62,63"}},{"content":"<strong>AlexNet</strong>: Rede neural profunda que venceu a competição de reconhecimento de imagens em 2012, popularizando as CNNs.","children":[],"payload":{"lines":"63,64"}},{"content":"<strong>VGGNet</strong>: Uma arquitetura simples e profunda, popular por sua simplicidade e eficácia em tarefas de visão computacional.","children":[],"payload":{"lines":"64,65"}},{"content":"<strong>ResNet</strong>: Rede neural com \"residual connections\" que permite o treinamento de redes muito profundas.","children":[],"payload":{"lines":"65,66"}},{"content":"<strong>Inception</strong>: Arquitetura que utiliza diferentes tamanhos de filtros em uma mesma camada, aumentando a capacidade de captura de características.","children":[],"payload":{"lines":"66,67"}},{"content":"<strong>MobileNet</strong>: Rede neural leve para dispositivos móveis, otimizada para velocidade e eficiência.","children":[],"payload":{"lines":"67,68"}},{"content":"<strong>EfficientNet</strong>: Arquitetura de rede neural que busca a melhor combinação de profundidade, largura e resolução para otimização de performance.","children":[],"payload":{"lines":"68,70"}}],"payload":{"lines":"61,62"}},{"content":"Redes Neurais Recorrentes (RNN)","children":[{"content":"<strong>Vanilla RNN</strong>: Redes neurais simples para modelar dados sequenciais, com problemas de longo prazo devido ao gradiente desaparecendo.","children":[],"payload":{"lines":"71,72"}},{"content":"<strong>Long Short-Term Memory (LSTM)</strong>: Tipo de RNN que resolve o problema de gradiente desaparecendo, permitindo aprender dependências de longo prazo.","children":[],"payload":{"lines":"72,73"}},{"content":"<strong>Gated Recurrent Unit (GRU)</strong>: Variante do LSTM, mais simples e eficiente, com uma arquitetura ligeiramente diferente.","children":[],"payload":{"lines":"73,74"}},{"content":"<strong>Bidirectional RNN</strong>: RNNs que processam as entradas em ambas as direções, melhorando o aprendizado em tarefas sequenciais.","children":[],"payload":{"lines":"74,76"}}],"payload":{"lines":"70,71"}},{"content":"Modelos de Atenção e Transformers","children":[{"content":"<strong>Transformer</strong>: Modelo de atenção que elimina a necessidade de redes recorrentes, sendo a base para modelos como BERT e GPT.","children":[],"payload":{"lines":"77,78"}},{"content":"<strong>BERT (Bidirectional Encoder Representations from Transformers)</strong>: Modelo de linguagem pré-treinado que captura contextos de forma bidirecional.","children":[],"payload":{"lines":"78,79"}},{"content":"<strong>GPT (Generative Pre-trained Transformer)</strong>: Modelo de linguagem autoregressivo, treinado para gerar texto.","children":[],"payload":{"lines":"79,80"}},{"content":"<strong>T5 (Text-to-Text Transfer Transformer)</strong>: Modelo unificado que trata todas as tarefas de NLP como problemas de conversão de texto para texto.","children":[],"payload":{"lines":"80,81"}},{"content":"<strong>XLNet</strong>: Modelo baseado em Transformer que considera permutações de palavras para aprender representações mais gerais.","children":[],"payload":{"lines":"81,82"}},{"content":"<strong>RoBERTa</strong>: Versão otimizada do BERT com mais dados e treinamento mais robusto.","children":[],"payload":{"lines":"82,84"}}],"payload":{"lines":"76,77"}}],"payload":{"lines":"55,56"}},{"content":"5. Algoritmos de Processamento de Linguagem Natural (NLP)","children":[{"content":"<strong>Word2Vec</strong>: Modelo que aprende representações vetoriais de palavras a partir de seu contexto, usado para capturar semântica de palavras.","children":[],"payload":{"lines":"88,89"}},{"content":"<strong>GloVe (Global Vectors for Word Representation)</strong>: Similar ao Word2Vec, mas baseia-se em uma matriz de co-ocorrência de palavras no corpus.","children":[],"payload":{"lines":"89,90"}},{"content":"<strong>TF-IDF (Term Frequency-Inverse Document Frequency)</strong>: Técnica para medir a importância de uma palavra em um documento, considerando sua frequência e raridade.","children":[],"payload":{"lines":"90,91"}},{"content":"<strong>Latent Dirichlet Allocation (LDA)</strong>: Modelo de tópicos que identifica tópicos ocultos em um conjunto de documentos.","children":[],"payload":{"lines":"91,92"}},{"content":"<strong>Named Entity Recognition (NER)</strong>: Técnica de extração de informações, usada para identificar entidades nomeadas em textos (ex.: pessoas, lugares).","children":[],"payload":{"lines":"92,93"}},{"content":"<strong>Bag-of-Words (BoW)</strong>: Representação de texto onde o contexto das palavras é ignorado, focando apenas na contagem de palavras.","children":[],"payload":{"lines":"93,94"}},{"content":"<strong>FastText</strong>: Modelo semelhante ao Word2Vec, mas que representa palavras como n-gramas, melhorando o tratamento de palavras raras.","children":[],"payload":{"lines":"94,95"}},{"content":"<strong>Sentence Transformers</strong>: Geração de representações vetoriais de sentenças completas para tarefas de semântica.","children":[],"payload":{"lines":"95,96"}},{"content":"<strong>ELMo (Embeddings from Language Models)</strong>: Geração de embeddings contextuais de palavras, levando em consideração o contexto da frase.","children":[],"payload":{"lines":"96,98"}}],"payload":{"lines":"86,87"}},{"content":"6. Algoritmos de Visualização de Dados","children":[{"content":"<strong>t-SNE (t-Distributed Stochastic Neighbor Embedding)</strong>: Técnica de visualização que preserva a proximidade entre pontos em alta dimensão, útil para visualizações 2D/3D.","children":[],"payload":{"lines":"102,103"}},{"content":"<strong>UMAP (Uniform Manifold Approximation and Projection)</strong>: Alternativa ao t-SNE, mais eficiente e escalável.","children":[],"payload":{"lines":"103,104"}},{"content":"<strong>PCA (Principal Component Analysis)</strong>: Usado para reduzir a dimensionalidade dos dados, projetando-os em componentes principais.","children":[],"payload":{"lines":"104,105"}}],"payload":{"lines":"100,101"}}],"payload":{"lines":"0,1"}},{"initialExpandLevel":0})</script>
</body>
</html>
